{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:12:08.543846Z","iopub.execute_input":"2025-04-05T08:12:08.544126Z","iopub.status.idle":"2025-04-05T08:12:08.933684Z","shell.execute_reply.started":"2025-04-05T08:12:08.544101Z","shell.execute_reply":"2025-04-05T08:12:08.932645Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# AcceletEd: Personalized AI Study Assistant 📚🤖  \n### Gen AI Intensive Course Capstone | 2025 Q1  \n**By:** Shankar Irla  \n**Powered by:** Transformers, FAISS, and Embeddings (Offline)\n\n---\n\nWelcome to **AcceletEd**, your personalized AI assistant designed to help you understand and recall complex concepts quickly using **Retrieval-Augmented Generation (RAG)**.  \nThis notebook demonstrates how to build a **fully offline RAG-based study assistant** using:\n\n- 🤗 HuggingFace Transformers (Offline GPT2)\n- 🧠 SentenceTransformers (Embeddings)\n- 🧭 FAISS (Similarity Search)\n\nLet’s get started! 🚀\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers sentence-transformers faiss-cpu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:13:54.016758Z","iopub.execute_input":"2025-04-05T08:13:54.017203Z","iopub.status.idle":"2025-04-05T08:14:02.007598Z","shell.execute_reply.started":"2025-04-05T08:13:54.017168Z","shell.execute_reply":"2025-04-05T08:14:02.006466Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.10.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 1: Import Libraries 📦\nWe’ll import the libraries needed for loading models, generating embeddings, and performing similarity search.\n","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:15:46.327144Z","iopub.execute_input":"2025-04-05T08:15:46.327502Z","iopub.status.idle":"2025-04-05T08:16:16.000221Z","shell.execute_reply.started":"2025-04-05T08:15:46.327476Z","shell.execute_reply":"2025-04-05T08:16:15.999123Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Step 2: Load Offline Models 🧠  \nWe’ll use:\n- **GPT-2** for local text generation\n- **MiniLM** for embedding context passages\nThese models work entirely offline after being downloaded.\n","metadata":{}},{"cell_type":"code","source":"# Load text generation model (offline)\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\n\n# Load embedding model\nfrom sentence_transformers import SentenceTransformer\nembed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:17:53.211042Z","iopub.execute_input":"2025-04-05T08:17:53.211428Z","iopub.status.idle":"2025-04-05T08:17:55.912889Z","shell.execute_reply.started":"2025-04-05T08:17:53.211399Z","shell.execute_reply":"2025-04-05T08:17:55.911903Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### 🔌 Model Initialization Complete!\nBoth the GPT-2 text generation model and SentenceTransformer (all-MiniLM-L6-v2) embedding model have been loaded successfully using CPU.\n\nWe'll now move on to defining our RAG pipeline logic.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Knowledge Base 📘  \nHere’s where we store your study material.  \nYou can later replace this with notes, textbook paragraphs, or even parsed PDF content.\n","metadata":{}},{"cell_type":"code","source":"docs = [\n    \"Python is a high-level programming language used for AI and Data Science.\",\n    \"Transformers are neural network architectures used for NLP tasks.\",\n    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n    \"LangChain helps in chaining LLM prompts, tools, and memory for dynamic agents.\",\n    \"Retrieval-Augmented Generation uses a retriever to feed relevant context to a generator.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:21:16.181798Z","iopub.execute_input":"2025-04-05T08:21:16.182162Z","iopub.status.idle":"2025-04-05T08:21:16.186976Z","shell.execute_reply.started":"2025-04-05T08:21:16.182135Z","shell.execute_reply":"2025-04-05T08:21:16.185338Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Step 4: Create Embeddings & FAISS Index 🔎  \nWe’ll convert our study material into embeddings and store them in a FAISS index for fast retrieval.\n","metadata":{}},{"cell_type":"code","source":"# Convert documents to embeddings\ndoc_embeddings = embed_model.encode(docs)\n\n# Create FAISS index\nindex = faiss.IndexFlatL2(doc_embeddings[0].shape[0])\nindex.add(np.array(doc_embeddings))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:22:42.059306Z","iopub.execute_input":"2025-04-05T08:22:42.059659Z","iopub.status.idle":"2025-04-05T08:22:42.236757Z","shell.execute_reply.started":"2025-04-05T08:22:42.059633Z","shell.execute_reply":"2025-04-05T08:22:42.235720Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787f89b3f39f453cb51fe178635894e6"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Step 5: Ask a Question using RAG 🤖  \nLet’s test our assistant!  \nWe’ll:\n- Embed the user question\n- Retrieve the most relevant context\n- Generate an answer using GPT2\n","metadata":{}},{"cell_type":"code","source":"# User query\nquery = \"What is FAISS used for?\"\n\n# Embed query\nquery_vec = embed_model.encode([query])\n\n# Search top document\nD, I = index.search(np.array(query_vec), k=1)\nretrieved_context = docs[I[0][0]]\n\n# Prepare prompt and generate answer\nprompt = f\"Context: {retrieved_context}\\n\\nQ: {query}\\nA:\"\nresponse = generator(prompt, max_length=60, num_return_sequences=1)\n\nprint(\"📘 Retrieved Context:\", retrieved_context)\nprint(\"🤖 Answer:\", response[0]['generated_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:23:32.079779Z","iopub.execute_input":"2025-04-05T08:23:32.080170Z","iopub.status.idle":"2025-04-05T08:23:33.168184Z","shell.execute_reply.started":"2025-04-05T08:23:32.080140Z","shell.execute_reply":"2025-04-05T08:23:33.167096Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649f88da684548bea4ca2f2124b82014"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"📘 Retrieved Context: FAISS is a library for efficient similarity search and clustering of dense vectors.\n🤖 Answer: Context: FAISS is a library for efficient similarity search and clustering of dense vectors.\n\nQ: What is FAISS used for?\nA: FAISS uses a simple package to store both non-linear and linear-like vectors (i.e. a \"linear\" matrix with\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Step 6: Improve the Output (Optional) ✨  \nLet’s refine our output formatting and clean up generation responses for better clarity.\n","metadata":{}},{"cell_type":"code","source":"# Get raw output\nraw_answer = response[0]['generated_text']\n\n# Remove the context/prompt part from the answer\nclean_answer = raw_answer.replace(prompt, \"\").strip()\n\nprint(\"📘 Retrieved Context:\", retrieved_context)\nprint(\"🤖 Answer:\", clean_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:25:25.059868Z","iopub.execute_input":"2025-04-05T08:25:25.060279Z","iopub.status.idle":"2025-04-05T08:25:25.066832Z","shell.execute_reply.started":"2025-04-05T08:25:25.060249Z","shell.execute_reply":"2025-04-05T08:25:25.065558Z"}},"outputs":[{"name":"stdout","text":"📘 Retrieved Context: FAISS is a library for efficient similarity search and clustering of dense vectors.\n🤖 Answer: FAISS uses a simple package to store both non-linear and linear-like vectors (i.e. a \"linear\" matrix with\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## ✅ Conclusion  \nYou’ve just built a **lightweight, offline AI Study Assistant** using open-source tools — no cloud, no API keys, just smart local magic! 💡  \nThis project showcases the power of RAG (Retrieval-Augmented Generation) using your **own notes**.\n\n### 🔁 What You Can Add Next:\n- 📄 Upload and parse your own study PDFs  \n- 🖼️ Add a Gradio or Streamlit UI for ease of use  \n- 🔁 Support multi-turn conversations with memory  \n- 💾 Store context in a persistent vector store or database  \n- 📚 Train it on your full semester syllabus and Q&A dumps\n\n---\n\n### 📌 **Project Title:** *AcceletEd — Personalized AI Study Assistant*  \n**By:** Shankar Irla  \n**Powered By:** 🤖 Transformers, 🧠 FAISS, 📚 RAG, 💬 Sentence Transformers\n\n> 🧑‍💻 *“Active | Shine | Rise. Explore to Connect. Share Your World, Your Way.”*","metadata":{}},{"cell_type":"markdown","source":"# 🧠 AcceletEd: Personalized AI Study Assistant  \n### 📘 A GenAI Capstone Project by Shankar Irla\n\n---\n\n> Imagine having an AI assistant that doesn’t just chat — but actually *knows your notes*, understands your subjects, and works fully offline. No cloud. No API keys. Just smart, local magic.\n>  \n> That’s what I built with **AcceletEd** — a personalized RAG-based study assistant designed to empower students with on-demand learning support using their own material.\n\n---\n\n## 🎯 The Goal  \nTo create a **lightweight AI Study Assistant** that:\n- Works completely **offline** (even in a Kaggle notebook)\n- Can **answer questions** from user-provided study material\n- Is built using **open-source tools only**\n- Helps students study smarter using **GenAI + RAG**\n\n---\n\n## 🧱 Tools Used  \n- 🤖 HuggingFace Transformers (`GPT2`)\n- 💬 Sentence Transformers (`all-MiniLM-L6-v2`)\n- 🧠 FAISS (for similarity search)\n- 📚 Retrieval-Augmented Generation (RAG)\n- 🐍 Python (100% local, runs in Kaggle)\n\n---\n\n## 🧪 How It Works  \n1. **Input Study Material**  \n   I start by defining or loading study notes (can be PDF, text, scraped data, etc.)\n\n2. **Create Embeddings + FAISS Index**  \n   The notes are embedded into vectors using `SentenceTransformer` and stored in a FAISS index.\n\n3. **Ask a Question (RAG)**  \n   When a question is asked, the most relevant context is retrieved using similarity search, and a GPT2 model generates an answer based on that context.\n\n---\n\n## ⚡ Demo Output  \n```plaintext\nQ: What is FAISS used for?\n\n📘 Retrieved Context:\n\"FAISS is a library for efficient similarity search and clustering of dense vectors.\"\n\n🤖 Answer:\n\"FAISS uses a simple package to store both non-linear and linear-like vectors (i.e. a linear matrix...)\"\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}